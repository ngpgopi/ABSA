{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import feature_extraction, model_selection, metrics, naive_bayes\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "# import gensim\n",
    "# from gensim.models.word2vec import Word2Vec\n",
    "import warnings\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "# from senticnet.senticnet import SenticNet\n",
    "from sentic import SenticPhrase\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read lexicon file \n",
    "fpositive = open('positive-words.txt','r',encoding = \"ISO-8859-1\")\n",
    "positive_words = []\n",
    "for each in fpositive:\n",
    "    positive_words.append(each[:-1])\n",
    "negative_words = []\n",
    "fnegative = open('negative-words.txt','r', encoding = \"ISO-8859-1\")\n",
    "for each1 in fnegative:\n",
    "    negative_words.append(each1[:-1])\n",
    "# print(positive_words[0])\n",
    "# print(negative_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrainDataandSample(data,dosampling,rate):\n",
    "    ## Import csv using pandas\n",
    "    df = pd.read_csv(data,index_col=0)\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    df.info()\n",
    "    \n",
    "    if(dosampling):\n",
    "        target_count = df[' class'].value_counts()\n",
    "        print(\"Before Sampling: \",target_count)\n",
    "        \n",
    "        max_size = (df[' class'].value_counts().max())\n",
    "\n",
    "        lst = [df]\n",
    "        for class_index, group in df.groupby(' class'):\n",
    "            lst.append(group.sample(int(abs(max_size-len(group))/rate), replace=True))\n",
    "        frame_new = pd.concat(lst)\n",
    "\n",
    "        target_count = frame_new[' class'].value_counts()\n",
    "        print(\"After sampling:\",target_count)\n",
    "\n",
    "#         target_count.plot(kind='bar', title='Count (target)')\n",
    "\n",
    "        sentence = frame_new[' text']\n",
    "        aspectLocation = frame_new[' term_location']\n",
    "        aspectTerm = frame_new[' aspect_term']\n",
    "        y = frame_new[' class']\n",
    "\n",
    "        df = pd.concat([sentence,aspectLocation,aspectTerm,y],axis=1)\n",
    "        df.reset_index()\n",
    "        df\n",
    "        df.to_csv('finalizedText.csv')\n",
    "    else:\n",
    "        target_count = df[' class'].value_counts()\n",
    "#         target_count.plot(kind='bar', title='Count (target)')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleaning(sentence):\n",
    "    # def dataCleaning(sentence):\n",
    "    sentenceCopy = sentence.copy()\n",
    "    # y = y.copy()\n",
    "    # Lower case words\n",
    "    sentenceCopy = sentenceCopy.apply(lambda x: x.lower())\n",
    "    # Replace comma with ,\n",
    "    sentenceCopy = sentenceCopy.apply(lambda x: x.replace('[comma]', ','))\n",
    "    # Remove numbers\n",
    "    table = str.maketrans(dict.fromkeys(string.digits))\n",
    "    sentenceCopy = sentenceCopy.apply(lambda x: x.translate(table))\n",
    "    # Remove Punctuations\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    sentenceCopy = sentenceCopy.apply(lambda x: x.translate(table))\n",
    "    \n",
    "    return sentenceCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove stop words and store each word\n",
    "def removeStopwords(sentence):\n",
    "    global df1\n",
    "    sentenceCopy = sentence\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    inputSentenceWords = []\n",
    "    # aspect = [] \n",
    "    for i,each in enumerate(sentenceCopy):\n",
    "        tokendWords = word_tokenize(each)\n",
    "#         if(len(tokendWords) <=500):\n",
    "        finalWords = [w for w in tokendWords if w not in stop_words]\n",
    "        inputSentenceWords.append(finalWords)\n",
    "#         else:\n",
    "#             print(i)\n",
    "#             df1.drop(df1[' class'][i])\n",
    "    return inputSentenceWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to input sentence\n",
    "def convertListofwords(inputSentenceWords):\n",
    "    finalInputSentence = []\n",
    "    for eachArray in inputSentenceWords:\n",
    "        x = ' '.join(word for word in eachArray)\n",
    "        finalInputSentence.append(x)\n",
    "    return finalInputSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon(inputSentenceWords):\n",
    "    global aspectTerm\n",
    "    lexicon = []\n",
    "    mat = []\n",
    "    for i,eachSentence in enumerate(inputSentenceWords):\n",
    "        total = 0\n",
    "        as1 = str(aspectTerm[i])\n",
    "        modifiedAspect = as1.split(' ')\n",
    "        q = 0\n",
    "        c = 0\n",
    "    #     print(i)\n",
    "        for eachWord in eachSentence:\n",
    "            if(eachWord not in modifiedAspect):\n",
    "                sp = SenticPhrase(eachWord)\n",
    "                q += sp.get_polarity(eachWord)\n",
    "                c += 1 \n",
    "                if(eachWord in positive_words):\n",
    "                    total += 1\n",
    "                elif(eachWord in negative_words):\n",
    "                    total -= 1\n",
    "                else:\n",
    "                    total += 0\n",
    "        lexicon.append(total)\n",
    "    #     q = 1\n",
    "    #     c = 1\n",
    "        if(c != 0):\n",
    "            mat.append(total)\n",
    "        else:\n",
    "            mat.append(0)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDFVector(finalInputSentence,mind,nrange):\n",
    "    vec = feature_extraction.text.TfidfVectorizer(min_df = mind, max_df = 0.8, sublinear_tf=True, use_idf=True, analyzer= 'word', ngram_range=(1,nrange),lowercase=True)\n",
    "    trainDataVecs = vec.fit_transform(finalInputSentence)\n",
    "#     print(type(trainDataVecs))\n",
    "    tempMatrix = trainDataVecs.toarray()\n",
    "    print(tempMatrix.shape)\n",
    "#     print(type(tempMatrix))\n",
    "    return trainDataVecs , tempMatrix , vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(idno,preds,oneortwo):\n",
    "    a = collections.Counter(preds)\n",
    "    print(\"0 count: \" , a[0])\n",
    "    print(\"1 count: \" , a[1])\n",
    "    print(\"-1 count: \" , a[-1])\n",
    "    \n",
    "    if (oneortwo == 1):\n",
    "        output = open('Mrudula_Borkar_GopiKrishnan_NarasimhaGuptha_Data-1.txt','w')\n",
    "    else:\n",
    "        output = open('Mrudula_Borkar_GopiKrishnan_NarasimhaGuptha_Data-2.txt','w')    \n",
    "    for i,y in enumerate(preds):\n",
    "        output.write(idno[i]+\";;\"+str(y)+'\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2203 entries, 0 to 2202\n",
      "Data columns (total 4 columns):\n",
      " text             2203 non-null object\n",
      " aspect_term      2203 non-null object\n",
      " term_location    2203 non-null object\n",
      " class            2203 non-null int64\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 68.9+ KB\n",
      "Before Sampling:   1    939\n",
      "-1    828\n",
      " 0    436\n",
      "Name:  class, dtype: int64\n",
      "After sampling:  1    939\n",
      "-1    850\n",
      " 0    536\n",
      "Name:  class, dtype: int64\n",
      "   Unnamed: 0                                               text  \\\n",
      "0           0  Obviously one of the most important features o...   \n",
      "1           1     Good for every day computing and web browsing.   \n",
      "2           2  while the keyboard itself is alright[comma] th...   \n",
      "3           3  Again[comma] the same problem[comma] the right...   \n",
      "4           4         My problem was with DELL Customer Service.   \n",
      "\n",
      "   term_location            aspect_term   class  \n",
      "0         69--84        human interface       0  \n",
      "1          9--28    every day computing       1  \n",
      "2       115--136  mouse command buttons      -1  \n",
      "3         29--42          right speaker      -1  \n",
      "4         20--41  DELL Customer Service      -1  \n",
      "(2325,)\n",
      "(2325,)\n",
      "2325\n",
      "accordingly i have decided to never purchase another hp product my five year old compaq lasted years before the hard drive crashed\n",
      "['accordingly', 'decided', 'never', 'purchase', 'another', 'hp', 'product', 'five', 'year', 'old', 'compaq', 'lasted', 'years', 'hard', 'drive', 'crashed']\n",
      "accordingly decided never purchase another hp product five year old compaq lasted years hard drive crashed\n",
      "(2325, 31782)\n",
      "(2325, 31783)\n",
      "(2325,)\n"
     ]
    }
   ],
   "source": [
    "readTrainDataandSample(\"data-1_train.csv\",True,5)\n",
    "\n",
    "df1 = pd.read_csv(\"finalizedText.csv\")\n",
    "print(df1.head())\n",
    "\n",
    "# df1 = readTrainDataandSample(\"data-1_train.csv\",False,0)\n",
    "\n",
    "sentence = df1[' text']\n",
    "sentenceCopy = dataCleaning(sentence)\n",
    "\n",
    "inputSentenceWords = removeStopwords(sentenceCopy)\n",
    "\n",
    "sentence = df1[' text']\n",
    "aspectTerm = df1[' aspect_term']\n",
    "y = df1[' class']\n",
    "\n",
    "print(y.shape)\n",
    "print(aspectTerm.shape)\n",
    "print(len(inputSentenceWords))\n",
    "\n",
    "print(sentenceCopy[6])\n",
    "\n",
    "print(inputSentenceWords[6])\n",
    "\n",
    "finalInputSentence = convertListofwords(inputSentenceWords)\n",
    "\n",
    "print(finalInputSentence[6])\n",
    "\n",
    "matasarr = np.asarray(lexicon(inputSentenceWords))\n",
    "# trainDataVecs , tempMatrix, idfmode11 = getTFIDFVector(finalInputSentence,0.000125,4)\n",
    "trainDataVecs , tempMatrix, idfmode11 = getTFIDFVector(finalInputSentence,0.000125,4)\n",
    "\n",
    "\n",
    "Xtrain = np.zeros((tempMatrix.shape[0],tempMatrix.shape[1]+1),dtype=float)\n",
    "\n",
    "for i in range(matasarr.shape[0]):\n",
    "    Xtrain[i] = np.hstack((tempMatrix[i],matasarr[i]))\n",
    "print(Xtrain.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVM to training data....\n",
      "\n",
      "Overall Acurracy - SVM:  0.753978494623656 \n",
      "\n",
      "Precision of -1 class: 0.725079\n",
      "Recall of -1 class: 0.812941\n",
      "F1-Score of -1 class: 0.766500 \n",
      "\n",
      "Precision of 0 class: 0.735714\n",
      "Recall of 0 class: 0.576493\n",
      "F1-Score of 0 class: 0.646444 \n",
      "\n",
      "Precision of 1 class: 0.790966\n",
      "Recall of 1 class: 0.801917\n",
      "F1-Score of 1 class: 0.796404 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=0,\n",
       "     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM on data 1\n",
    "\n",
    "clfData1=LinearSVC(multi_class='crammer_singer',random_state=0)\n",
    "print(\"Fitting SVM to training data....\")    \n",
    "preds = model_selection.cross_val_predict(clfData1, Xtrain, y, cv=10)\n",
    "# print(preds)\n",
    "accScore = metrics.accuracy_score(y,preds)\n",
    "labels = [-1, 0, 1]\n",
    "precision = metrics.precision_score(y,preds,average=None,labels=labels)\n",
    "recall = metrics.recall_score(y,preds,average=None,labels=labels)\n",
    "f1Score = metrics.f1_score(y,preds,average=None,labels=labels)\n",
    "print(\"\\nOverall Acurracy - SVM: \",accScore,\"\\n\")\n",
    "for i in range(len(labels)):\n",
    "    print(\"Precision of %s class: %f\" %(labels[i],precision[i]))\n",
    "    print(\"Recall of %s class: %f\" %(labels[i],recall[i]))\n",
    "    print(\"F1-Score of %s class: %f\" %(labels[i],f1Score[i]),\"\\n\")\n",
    "clfData1.fit(Xtrain, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3602 entries, 0 to 3601\n",
      "Data columns (total 4 columns):\n",
      " text             3602 non-null object\n",
      " aspect_term      3602 non-null object\n",
      " term_location    3602 non-null object\n",
      " class            3602 non-null int64\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 112.6+ KB\n",
      "Before Sampling:   1    2164\n",
      "-1     805\n",
      " 0     633\n",
      "Name:  class, dtype: int64\n",
      "After sampling:  1    2164\n",
      "-1    1144\n",
      " 0    1015\n",
      "Name:  class, dtype: int64\n",
      "   Unnamed: 0                                               text  \\\n",
      "0           0               But the staff was so horrible to us.   \n",
      "1           1  To be completely fair[comma] the only redeemin...   \n",
      "2           2  The food is uniformly exceptional[comma] with ...   \n",
      "3           3  The food is uniformly exceptional[comma] with ...   \n",
      "4           4  The food is uniformly exceptional[comma] with ...   \n",
      "\n",
      "   term_location  aspect_term   class  \n",
      "0          8--13        staff      -1  \n",
      "1         57--61         food       1  \n",
      "2           4--8         food       1  \n",
      "3         55--62      kitchen       1  \n",
      "4       141--145         menu       0  \n",
      "(4323,)\n",
      "(4323,)\n",
      "4323\n",
      "not only was the food outstanding but the little perks were great\n",
      "['food', 'outstanding', 'little', 'perks', 'great']\n",
      "food outstanding little perks great\n",
      "(4323, 26932)\n",
      "(4323, 26933)\n",
      "(4323,)\n"
     ]
    }
   ],
   "source": [
    "readTrainDataandSample(\"data-2_train.csv\",True,4)\n",
    "\n",
    "df1 = pd.read_csv(\"finalizedText.csv\")\n",
    "print(df1.head())\n",
    "\n",
    "# df1 = readTrainDataandSample(\"data-2_train.csv\",False,0)\n",
    "\n",
    "sentence = df1[' text']\n",
    "sentenceCopy = dataCleaning(sentence)\n",
    "\n",
    "inputSentenceWords = removeStopwords(sentenceCopy)\n",
    "\n",
    "sentence = df1[' text']\n",
    "aspectTerm = df1[' aspect_term']\n",
    "y = df1[' class']\n",
    "\n",
    "print(y.shape)\n",
    "print(aspectTerm.shape)\n",
    "print(len(inputSentenceWords))\n",
    "\n",
    "print(sentenceCopy[6])\n",
    "\n",
    "print(inputSentenceWords[6])\n",
    "\n",
    "finalInputSentence = convertListofwords(inputSentenceWords)\n",
    "\n",
    "print(finalInputSentence[6])\n",
    "\n",
    "matasarr = np.asarray(lexicon(inputSentenceWords))\n",
    "trainDataVecs , tempMatrix , idfmode12 = getTFIDFVector(finalInputSentence,0.000125,3)\n",
    "\n",
    "Xtrain = np.zeros((tempMatrix.shape[0],tempMatrix.shape[1]+1),dtype=float)\n",
    "\n",
    "for i in range(matasarr.shape[0]):\n",
    "    Xtrain[i] = np.hstack((tempMatrix[i],matasarr[i]))\n",
    "print(Xtrain.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVM to training data....\n",
      "\n",
      "Overall Acurracy - SVM:  0.7228776312745778 \n",
      "\n",
      "Precision of -1 class: 0.661803\n",
      "Recall of -1 class: 0.673951\n",
      "F1-Score of -1 class: 0.667822 \n",
      "\n",
      "Precision of 0 class: 0.636457\n",
      "Recall of 0 class: 0.608867\n",
      "F1-Score of 0 class: 0.622356 \n",
      "\n",
      "Precision of 1 class: 0.793781\n",
      "Recall of 1 class: 0.802218\n",
      "F1-Score of 1 class: 0.797977 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=0,\n",
       "     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM on data 2\n",
    "\n",
    "clfData2=LinearSVC(multi_class='crammer_singer',random_state=0)\n",
    "print(\"Fitting SVM to training data....\")    \n",
    "preds = model_selection.cross_val_predict(clfData2, Xtrain, y, cv=10)\n",
    "# print(preds)\n",
    "accScore = metrics.accuracy_score(y,preds)\n",
    "labels = [-1, 0, 1]\n",
    "precision = metrics.precision_score(y,preds,average=None,labels=labels)\n",
    "recall = metrics.recall_score(y,preds,average=None,labels=labels)\n",
    "f1Score = metrics.f1_score(y,preds,average=None,labels=labels)\n",
    "print(\"\\nOverall Acurracy - SVM: \",accScore,\"\\n\")\n",
    "for i in range(len(labels)):\n",
    "    print(\"Precision of %s class: %f\" %(labels[i],precision[i]))\n",
    "    print(\"Recall of %s class: %f\" %(labels[i],recall[i]))\n",
    "    print(\"F1-Score of %s class: %f\" %(labels[i],f1Score[i]),\"\\n\")\n",
    "clfData2.fit(Xtrain, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(638,)\n",
      "(638,)\n",
      "638\n",
      "chatting with acer support i was advised the problem was corrupted operating system files\n",
      "['chatting', 'acer', 'support', 'advised', 'problem', 'corrupted', 'operating', 'system', 'files']\n",
      "chatting acer support advised problem corrupted operating system files\n",
      "(638, 31782)\n",
      "(638, 31783)\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(data,index_col=0)\n",
    "df1test = pd.read_csv(\"Data-1_test.csv\",index_col=0)\n",
    "df1test[:5]\n",
    "idno = df1test.index.values\n",
    "sentence1 = df1test[' text']\n",
    "aspectTerm1 = df1test[' aspect_term']\n",
    "print(sentence1.shape)\n",
    "\n",
    "sentenceCopy1 = dataCleaning(sentence1)\n",
    "\n",
    "inputSentenceWords1 = removeStopwords(sentenceCopy1)\n",
    "\n",
    "print(aspectTerm1.shape)\n",
    "print(len(inputSentenceWords1))\n",
    "\n",
    "print(sentenceCopy1[6])\n",
    "\n",
    "print(inputSentenceWords1[6])\n",
    "\n",
    "finalInputSentence1 = convertListofwords(inputSentenceWords1)\n",
    "\n",
    "print(finalInputSentence1[6])\n",
    "\n",
    "\n",
    "matasarr = np.asarray(lexicon(inputSentenceWords1))\n",
    "\n",
    "testDataVecs1 = idfmode11.transform(finalInputSentence1)\n",
    "tempMatrix = testDataVecs1.toarray()\n",
    "print(tempMatrix.shape)\n",
    "\n",
    "Xtest1 = np.zeros((tempMatrix.shape[0],tempMatrix.shape[1]+1),dtype=float)\n",
    "\n",
    "for i in range(matasarr.shape[0]):\n",
    "    Xtest1[i] = np.hstack((tempMatrix[i],matasarr[i]))\n",
    "print(Xtest1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 count:  51\n",
      "1 count:  373\n",
      "-1 count:  214\n"
     ]
    }
   ],
   "source": [
    "# predict on SVM\n",
    "preds = clfData1.predict(Xtest1)\n",
    "write(idno,preds,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1120,)\n",
      "(1120,)\n",
      "1120\n",
      "food is excellent and they also have empenadas and plaintains which are good for an afternoon snack\n",
      "['food', 'excellent', 'also', 'empenadas', 'plaintains', 'good', 'afternoon', 'snack']\n",
      "food excellent also empenadas plaintains good afternoon snack\n",
      "(1120, 26932)\n",
      "(1120, 26933)\n"
     ]
    }
   ],
   "source": [
    "df2test = pd.read_csv(\"Data-2_test.csv\",index_col=0)\n",
    "df2test[:5]\n",
    "idno2 = df2test.index.values\n",
    "sentence2 = df2test[' text']\n",
    "aspectTerm2 = df2test[' aspect_term']\n",
    "print(sentence2.shape)\n",
    "\n",
    "sentenceCopy2 = dataCleaning(sentence2)\n",
    "\n",
    "inputSentenceWords2 = removeStopwords(sentenceCopy2)\n",
    "\n",
    "print(aspectTerm2.shape)\n",
    "print(len(inputSentenceWords2))\n",
    "\n",
    "print(sentenceCopy2[6])\n",
    "\n",
    "print(inputSentenceWords2[6])\n",
    "\n",
    "finalInputSentence2 = convertListofwords(inputSentenceWords2)\n",
    "\n",
    "print(finalInputSentence2[6])\n",
    "\n",
    "\n",
    "matasarr = np.asarray(lexicon(inputSentenceWords2))\n",
    "\n",
    "testDataVecs2 = idfmode12.transform(finalInputSentence2)\n",
    "tempMatrix = testDataVecs2.toarray()\n",
    "print(tempMatrix.shape)\n",
    "\n",
    "Xtest2 = np.zeros((tempMatrix.shape[0],tempMatrix.shape[1]+1),dtype=float)\n",
    "\n",
    "for i in range(matasarr.shape[0]):\n",
    "    Xtest2[i] = np.hstack((tempMatrix[i],matasarr[i]))\n",
    "print(Xtest2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 count:  65\n",
      "1 count:  834\n",
      "-1 count:  221\n"
     ]
    }
   ],
   "source": [
    "# predict on SVM\n",
    "preds = clfData2.predict(Xtest2)\n",
    "write(idno2,preds,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
